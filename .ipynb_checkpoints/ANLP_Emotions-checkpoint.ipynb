{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-Récupération des jeux données : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Emotion_final.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e1dd8b913436>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdatak\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Emotion_final.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdataw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text_emotion.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nidhal/anaconda3/envs/plotly/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nidhal/anaconda3/envs/plotly/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nidhal/anaconda3/envs/plotly/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nidhal/anaconda3/envs/plotly/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nidhal/anaconda3/envs/plotly/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Emotion_final.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "datak = pd.read_csv('Emotion_final.csv')\n",
    "dataw = pd.read_csv('text_emotion.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affichage des datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"shape of datak = \",datak.shape)\n",
    "datak.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"shape of dataw = \",dataw.shape)\n",
    "dataw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affichage des labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datak_np = datak.Emotion.to_numpy()\n",
    "np.unique(datak_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(dataw.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets1 = list(datak[\"Emotion\"])\n",
    "corpus1 = list(datak[\"Text\"])\n",
    "\n",
    "targets2 = list(dataw[\"sentiment\"])\n",
    "corpus2 = list(dataw[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Preprocessing & Visualization des données :  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Le preprocessing comporte plusieurs étapes souvent complémentaires. Nous allons ici découvrir les plus courantes, à savoir :\n",
    "\n",
    "-Normalisation du texte : le mettre à la même casse, souvent tout en minuscule.\n",
    "-Tokénization : divise une chaîne de caractère en tokens\n",
    "-Suppression des stopwords : enlever dans le texte tous les mots qui n’ont que peu d’intérêt sémantique\n",
    "-Lemmatization (ou Stemming) : Stemming=uniquement garder le radical\n",
    "-N-grams : des suites de mots présents dans le texte\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import plotly.express as px\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. Pie des émotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a table that counts emotions frequencies\n",
    "\n",
    "list_labels = ['anger', 'fear', 'happy', 'love', 'sadness', 'surprise']\n",
    "list_freq = [0,0,0,0,0,0]\n",
    "# df10 = pd.DataFrame(columns=('Emotion', 'Freq'))\n",
    "for i in range(datak.shape[0]):\n",
    "    ind = list_labels.index(datak[\"Emotion\"][i])\n",
    "    list_freq[ind] += 1 \n",
    "# df10.loc[new_index[i]] = [cm1[i,0], cm1[i,1], cm1[i,2],cm1[i,3],cm1[i,4],cm1[i,5], list_labels[m_index]]\n",
    "print(list_freq)\n",
    "# Pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
    "\n",
    "explode = (0, 0, 0.15, 0, 0, 0)  # only \"explode\" the 4nd slice \n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(list_freq, explode=explode, labels=list_labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. Histograme des émotions (Premier jeu de données)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(targets1, bins=6, color='c', edgecolor='k', alpha=0.65)\n",
    "plt.title('Histogramme des émotions')\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('N_Sentences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. Tokénisation et affichage des mots et des Ngrams (Premier jeu de données)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "def tokenize(corpus, n):\n",
    "    #Tokenization\n",
    "    words=[]\n",
    "    for i in range(len(corpus)):\n",
    "        tokens = nltk.word_tokenize(corpus[i])\n",
    "        words.append(tokens)\n",
    "    flat_words = [item for sublist in words for item in sublist]\n",
    "    \n",
    "    stopW = stopwords.words('english')\n",
    "    stopW.extend(set(string.punctuation))\n",
    "    tokens_without_stopwords = [x for x in flat_words if x not in stopW]\n",
    "    n_grams = ngrams(tokens_without_stopwords,n)\n",
    "    \n",
    "    return n_grams\n",
    "\n",
    "\n",
    "def plot_fig(ngrams):\n",
    "    #ploting with seaborn\n",
    "    sns.set(rc={'figure.figsize' : (11,4)})\n",
    "    sns.set_style('darkgrid')\n",
    "    nlp_words = nltk.FreqDist(ngrams)\n",
    "    nlp_words.plot(20)\n",
    "    \n",
    "def get_top_words_or_ngrams(corpus, n):\n",
    "    tokens_ngrams = tokenize(corpus, n)  \n",
    "    fig = plot_fig(tokens_ngrams)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "# import numpy as np\n",
    "# x = np.array([1, 2, 3, 4, 5])\n",
    "# y = np.array([1, 3, 2, 3, 1])\n",
    "\n",
    "# fig = go.Figure()\n",
    "\n",
    "# fig.add_trace(go.Scatter(x=x, y=y + 5, name=\"spline\",\n",
    "#                     text=[\"tweak line smoothness<br>with 'smoothing' in line object\"],\n",
    "#                     hoverinfo='text+name',\n",
    "#                     line_shape='spline'))\n",
    "\n",
    "# fig.update_traces(hoverinfo='text+name', mode='lines+markers')\n",
    "# fig.update_layout(legend=dict(y=0.5, traceorder='reversed', font_size=16))\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_some_ngrams(corpus, n):\n",
    "    l1 = []\n",
    "    l = []\n",
    "    k=0\n",
    "    tokens_ngrams = tokenize(corpus, n) \n",
    "    for i in tokens_ngrams : \n",
    "        for j in range(n):\n",
    "            l.append(str(i[j]))\n",
    "        l1.append(l)\n",
    "        if k==10 :\n",
    "            print(l1)\n",
    "            break\n",
    "        l.clear()\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_some_ngrams(corpus1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fréquences des apparutions (des mots et des Ngrams): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Les mots(premier jeu de données)\n",
    "get_top_words_or_ngrams(corpus1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Les bigrams (premier jeu de données)\n",
    "get_top_words_or_ngrams(corpus1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a\n",
    "def get_top_n_igram(corpus, dim, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(dim, dim), stop_words=stopwords.words(\"english\")).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "common_words = get_top_n_igram(corpus1,3, 10)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "\n",
    "df3 = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3.groupby('ReviewText').sum()['count'].sort_values(ascending=False).iplot(\n",
    "#     kind='bar', yTitle='Count', linecolor='black', title='Top 20 bigrams in review before removing stop words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. Histogramme des émotions avec Plotly (Deuxième jeu de données)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.histogram(x=targets2, nbins=20).update_xaxes(categoryorder = 'total descending')\n",
    "fig.update_layout( title=\"Histogramme des sentiments \",\n",
    "    xaxis_title=\"Sentiments\",\n",
    "    yaxis_title=\"N_Fois\")\n",
    "    \n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4. Ngrams exemple (Deuxieme jeu de données)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Tri_grams\n",
    "get_top_words_or_ngrams(corpus2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Vectorization et Apprentissage (corpus1) :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets1 = list(datak[\"Emotion\"].map({'anger':0, 'fear':1, 'happy':2, 'love':3, 'sadness':4, 'surprise':5}))\n",
    "corpus1 = list(datak[\"Text\"])\n",
    "\n",
    "targets2 = list(dataw[\"sentiment\"].map({'anger':0, 'worry':1, 'happiness':2, 'love':3, 'sadness':4, 'surprise':5,'neutral':6, 'fun':7, 'relief':8, 'hate': 9, 'empty':10, 'enthusiasm':11, 'boredom': 12 }))\n",
    "corpus2 = list(dataw[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create the transform\n",
    "Count_vectorizer = CountVectorizer( stop_words=stopwords.words(\"english\"))\n",
    "# tokenize and build vocab\n",
    "Count_vectorizer.fit(corpus1)\n",
    "# summarize\n",
    "#print(vectorizer.vocabulary_)\n",
    "# encode document\n",
    "vector = Count_vectorizer.transform(corpus1)\n",
    "# summarize encoded vector\n",
    "print(\"vector.shape = \",vector.shape)\n",
    "#print(type(vector))\n",
    "print(vector.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison entre counterVectorizer et TfidVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfid_vectorizer = TfidfVectorizer( stop_words=stopwords.words(\"english\"))\n",
    "vector = Tfid_vectorizer.fit_transform(corpus1)\n",
    "# summarize encoded vector\n",
    "print(\"vector.shape = \",vector.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Apprentissage : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning,module=\"sklearn\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Création des pipes : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = Pipeline([('vect', CountVectorizer()), ('sgd', SGDClassifier()),])\n",
    "pipe2 = Pipeline([('vect', CountVectorizer()),('svm', SVC()),])\n",
    "pipe3 = Pipeline([('vect', CountVectorizer()), ('logreg', LogisticRegression()),])\n",
    "pipe4 = Pipeline([('vect', CountVectorizer()),('KNN', KNeighborsClassifier(n_neighbors=4)),]) \n",
    "pipe5 = Pipeline([('vect', CountVectorizer()),('DTC', DecisionTreeClassifier(max_depth=4)),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipes(pipes) :\n",
    "    \n",
    "    df = pd.DataFrame(columns=['model', 'fitting_time', 'accuracy',  'precision', 'recall'])\n",
    "    \n",
    "    for pipe in pipes : \n",
    "        start = time()\n",
    "        pipe.fit(X_train, y_train)\n",
    "        fit_time = time() - start\n",
    "        y_pred =pipe.predict(X_test)\n",
    "        \n",
    "        acc = metrics.accuracy_score(y_test, y_pred) \n",
    "        pre = metrics.precision_score(y_test,y_pred,average ='macro', zero_division=0)\n",
    "        rec = metrics.recall_score(y_test,y_pred, average ='macro', zero_division=0)\n",
    "        #f1_score(y_true, y_pred, average='macro')\n",
    "        \n",
    "        df = df.append({'model':pipe.steps[1][0], 'fitting_time':fit_time, \n",
    "                        'accuracy': acc, 'precision':pre, 'recall': rec},ignore_index=True)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Evaluation des models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = corpus1\n",
    "y = targets1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=0)\n",
    "\n",
    "df1 = run_pipes(pipes= [pipe1,pipe2, pipe3, pipe4,pipe5])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df1.model, y=df1.accuracy, name=\"accuracy\",  line_shape='linear'))\n",
    "fig.add_trace(go.Scatter(x=df1.model, y=df1.precision, name=\"precision\",  line_shape='linear'))\n",
    "fig.add_trace(go.Scatter(x=df1.model, y=df1.recall, name=\"recall\",  line_shape='linear'))\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "save_classifier = open(\"logreg.pickle\",\"wb\")\n",
    "pickle.dump(pipe3, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_f = open(\"logreg.pickle\", \"rb\")\n",
    "classifier = pickle.load(classifier_f)\n",
    "classifier_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Matrice de confusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrice de confusion\n",
    "y_pred =pipe3.predict(X)\n",
    "cm1 = confusion_matrix(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_labels = ['anger', 'fear', 'happy', 'love', 'sadness', 'surprise']\n",
    "df10 = pd.DataFrame(columns=('anger', 'fear', 'happy', 'love', 'sadness', 'surprise', 'prediction_du_model'))\n",
    "for i in range(cm1.shape[0]):\n",
    "    list_pred=[cm1[i,0], cm1[i,1], cm1[i,2],cm1[i,3],cm1[i,4],cm1[i,5]]\n",
    "    max_value = max(list_pred)\n",
    "    m_index = list_pred.index(max_value)\n",
    "    df10.loc[list_labels[i]] = [cm1[i,0], cm1[i,1], cm1[i,2],cm1[i,3],cm1[i,4],cm1[i,5], list_labels[m_index]]\n",
    "print(df10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Test et évaluation des anciens models sur le deuxieme dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = corpus2\n",
    "y2 = targets2\n",
    "\n",
    "y_pred2 =pipe1.predict(X2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipes_with_data2(pipes) :\n",
    "    \n",
    "    df2 = pd.DataFrame(columns=['model',  'accuracy',  'precision', 'recall'])\n",
    "    \n",
    "    for pipe in pipes : \n",
    "       \n",
    "        y_pred =pipe.predict(X)\n",
    "        \n",
    "        acc = metrics.accuracy_score(y, y_pred) \n",
    "        pre = metrics.precision_score(y,y_pred,average ='macro', zero_division=0)\n",
    "        rec = metrics.recall_score(y,y_pred, average ='macro', zero_division=0)\n",
    "    \n",
    "        df2 = df2.append({'model':pipe.steps[1][0],  'accuracy': acc, 'precision':pre, 'recall': rec},ignore_index=True)\n",
    "        \n",
    "    return df2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = corpus2\n",
    "y = targets2\n",
    "\n",
    "df2= evaluate_pipes_with_data2(pipes= [pipe1, pipe3, pipe4,pipe5])\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# very poor scores !! \n",
    "\n",
    "#in the next, w'ill try to undrstand why our models are not capable to make good predictions when we try to predict things from corpus2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice de confusion (corpus2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred =pipe3.predict(X)\n",
    "cm2 = confusion_matrix(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "new_index = ['anger', 'worry', 'happiness', 'love', 'sadness', 'surprise','neutral', 'fun', 'relief', 'hate', 'empty', 'enthusiasm', 'boredom']\n",
    "list_labels = ['anger', 'worry', 'happiness', 'love', 'sadness', 'surprise']\n",
    "df = pd.DataFrame(columns=('anger', 'worry', 'happiness', 'love', 'sadness', 'surprise', 'replace_it_by'))\n",
    "for i in range(cm2.shape[0]):\n",
    "    list_pred=[cm2[i,0], cm2[i,1], cm2[i,2],cm2[i,3],cm2[i,4],cm2[i,5]]\n",
    "    max_value = max(list_pred)\n",
    "    m_index = list_pred.index(max_value)\n",
    "    df.loc[new_index[i]] = [cm2[i,0], cm2[i,1], cm2[i,2],cm2[i,3],cm2[i,4],cm2[i,5], list_labels[m_index]]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Selon les derniers résultats d'évaluation par matrice de confusion, on va remplacers les labels de la deuxième datasets par la prèdiction_max faite par le model\n",
    "=>sens inverse : au lieu de faire un entrainement d'un nouveau model, on voit les predictions d'un ancien model puis on adapte nos labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Modification des labels (selon previous predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'anger':0, 'fear':1, 'happy':2, 'love':3, 'sadness':4, 'surprise':5  (sur première dataset)\n",
    "# 'anger':0, 'worry':1, 'happiness':2, 'love':3, 'sadness':4, 'surprise':5 (sur deuxième dataset)\n",
    "targets2 = list(dataw[\"sentiment\"].map({'anger':4, 'worry':2, 'happiness':2, 'love':2, 'sadness':4, 'surprise':2,'neutral':2, 'fun':2, 'relief':2, 'hate': 4, 'empty':2, 'enthusiasm':2, 'boredom': 4 }))\n",
    "corpus2 = list(dataw[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = corpus2\n",
    "y = targets2\n",
    "\n",
    "df2= evaluate_pipes_with_data2(pipes= [pipe1, pipe3, pipe4,pipe5])\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conclusion : => amélioration claire des scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Concatenation des jeux de données et entrainement des nouveaux models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conatenation of dataframe and re-building of the models\n",
    "targets1 = list(datak[\"Emotion\"].map({'anger':0, 'fear':1, 'happy':2, 'love':3, 'sadness':4, 'surprise':5}))\n",
    "corpus1 = list(datak[\"Text\"])\n",
    "\n",
    "targets2 = list(dataw[\"sentiment\"].map({'anger':0, 'worry':1, 'happiness':2, 'love':3, 'sadness':4, 'surprise':5,'neutral':6, 'fun':7, 'relief':8, 'hate': 9, 'empty':10, 'enthusiasm':11, 'boredom': 12 }))\n",
    "corpus2 = list(dataw[\"content\"])\n",
    "\n",
    "corpus = corpus1+corpus2\n",
    "targets = targets1+targets2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = corpus\n",
    "y = targets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=0)\n",
    "\n",
    "\n",
    "df1= run_pipes(pipes= [pipe1, pipe3, pipe4,pipe5])\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### amélioration des résultats % à l'utilisation des premiers models pour prédire les labels du deuxieme dataset\n",
    "## presques meme résutats % utilisation des premiers models avec deuxieme dataset labels modifiés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = corpus\n",
    "y = targets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=0)\n",
    "\n",
    "pipe = Pipeline([('vect', CountVectorizer()), ('sgd', SGDClassifier()),])\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred =pipe.predict(X_test)\n",
    "\n",
    "\n",
    "# pick a string and test it \n",
    "labels = ['anger', 'worry', 'happiness', 'love', 'sadness', 'surprise','neutral', 'fun', 'relief', 'hate', 'empty', 'enthusiasm', 'boredom']\n",
    "ltext = []\n",
    "def get_text():\n",
    "  while True:\n",
    "    text = input(\"Pick a text: \")\n",
    "    if isinstance(text, str) :\n",
    "        return text\n",
    "    else:\n",
    "      print(\"Try again...\")\n",
    "\n",
    "text = get_text()\n",
    "ltext.append(text)\n",
    "y_pred =pipe1.predict(ltext)\n",
    "print(labels[y_pred[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class (OVR, Multinomial) en plusieurs itérations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_20newsgroups.html#sphx-glr-auto-examples-linear-model-plot-sparse-logistic-regression-20newsgroups-py\n",
    "import timeit\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.exceptions import ConvergenceWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take starting time\n",
    "t0 = timeit.default_timer()\n",
    "\n",
    "# We use SAGA solver\n",
    "solver = 'saga'\n",
    "\n",
    "X=vector\n",
    "y=targets1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42,   stratify=y, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_features = X_train.shape[0]\n",
    "n_features = X_train.shape[0]\n",
    "n_classes = len(np.unique(y))\n",
    "print(' n_features=%i, n_classes=%i' % ( n_features, n_classes))\n",
    "\n",
    "models = {\n",
    "    'ovr': {'name': 'One versus Rest', 'iters': [1, 2,5]},\n",
    "    'multinomial': {'name': 'Multinomial', 'iters': [1, 2,5]}   \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    # Add initial chance-level values for plotting purpose\n",
    "    accuracies = [1 / n_classes]\n",
    "    times = [0]\n",
    "    model_params = models[model]\n",
    "\n",
    "    # Small number of epochs for fast runtime\n",
    "    for this_max_iter in model_params['iters']:\n",
    "        print('[model=%s, solver=%s] Number of epochs: %s' % (model_params['name'], solver, this_max_iter))\n",
    "        lr = LogisticRegression(solver=solver,multi_class=model,penalty='l1',max_iter=this_max_iter,random_state=42,)\n",
    "        t1 = timeit.default_timer()\n",
    "        lr.fit(X_train, y_train)\n",
    "        train_time = timeit.default_timer() - t1\n",
    "\n",
    "        y_pred = lr.predict(X_test)\n",
    "        accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "        accuracies.append(accuracy)\n",
    "        times.append(train_time)\n",
    "    models[model]['times'] = times\n",
    "    models[model]['accuracies'] = accuracies\n",
    "    print(\"****** Resultats ******\")\n",
    "    print('Test accuracy for model %s: %.4f' % (model, accuracies[-1]))\n",
    "    print('Run time (%i epochs) for model %s: %.2f' % (model_params['iters'][-1], model, times[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "for model in models:\n",
    "    name = models[model]['name']\n",
    "    times = models[model]['times']\n",
    "    accuracies = models[model]['accuracies']\n",
    "    ax.plot(times, accuracies, marker='o', label='Model: %s' % name)\n",
    "    ax.set_xlabel('Train time (s)')\n",
    "    ax.set_ylabel('Test accuracy')\n",
    "ax.legend()\n",
    "fig.suptitle('Multinomial vs One-vs-Rest Logistic L1\\n')\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.85)\n",
    "run_time = timeit.default_timer() - t0\n",
    "print('Example run in %.3f s' % run_time)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=> Avec des accuracies equivalentes, le fitting du model est plus \"rapide\" avec le multinomial qu'avec OvR\n",
    "#=> entrainer le model sur plusier itértions fne fait pas améliorer l'accuracy dans notre cas, \n",
    "# => c'est mieux de daire une regression logistique on multinomial class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
